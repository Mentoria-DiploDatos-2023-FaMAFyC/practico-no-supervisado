{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practico 3: Aplicación de técnicas de aprendizaje automático no supervisado\n",
    "\n",
    "# Mentoría - Análisis de Películas\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este practico vamos a apoyarnos en el aprendizaje no supervisado para analizar nuestro dataset. El objetivo es profundizar la exploración, e intentar descubrir algún patron desconocido dentro de los datos que no hayamos podido ver en los primeros dos prácticos, o confirmar los que si se hayan visto.\n",
    "\n",
    "## Parte 0 - Selección de features\n",
    "\n",
    "Los features son las columnas del dataset en la etapa previa al entrenamiento de un modelo de aprendizaje automático. Esta etapa condiciona completamente a las siguientes. Cuando seleccionan un grupo de features de un dataset, lo hacen con una intención clara, y una pregunta para responder. En nuestro caso, queremos profundizar la exploración, buscando relaciones entre features que hayamos destacado o vimos interesantes en los prácticos anteriores. Cuando piensen en la relación que quieran explorar, compongan las features que crean que afectan a esa relación para ahi encontrar los clusters. Tienen que apoyarse en los análisis realizados para justificar la selección. \n",
    "\n",
    "### Consigna\n",
    "\n",
    "- Hagan 1 o mas selecciones de columnas para entrenar en un modelo no supervisado. Puede haber categóricas y/o numéricas.\n",
    "- Justifiquen la selección de columnas. Que enfoque quieren dar a la exploración?\n",
    "\n",
    "## Parte I - Limpieza de datos\n",
    "\n",
    "Es importante la limpieza de datos previa a entrenar el modelo. El análisis y la curación realizadas en prácticos anteriores es esencial para comprender el comportamiento del modelo. Si hay muchos outliers o datos erróneos, el modelo no va a funcionar como queremos que lo haga.\n",
    "\n",
    "Necesariamente, los features son arrays de colecciones numéricas (matrices o tensores). Esto significa que debemos codificar todas nuestras variables categóricas si queremos incluirlas en nuestro modelo. \n",
    "\n",
    "### Consigna\n",
    "\n",
    "- Todos los features seleccionados deben estar correctamente curados, filtrados, escalados de ser necesario si son numéricos, y codificados si son categóricos. **Esto implica que la cantidad total de filas del dataset resultante sera afectado por los filtros de cada feature seleccionado.** Reutilizar los filtros descritos en el practico anterior.\n",
    "\n",
    "## Parte II - PCA\n",
    "\n",
    "PCA es *principal component analysis*, y como lo dice el nombre, analiza los componentes principales de un conjunto de datos. Se torna útil cuando tenemos muchas columnas, y en nuestro caso posiblemente sea el caso cuando incluyamos las features categóricas con encoding.\n",
    "\n",
    "### Consignas\n",
    "\n",
    "- Apliquen escalado de 0 a 1 a las columnas numéricas\n",
    "- Codifiquen las columnas categóricas\n",
    "- Apliquen PCA sobre los features resultantes. Estudien y visualicen los resultados. **Describan lo que ven.** Recuerden que la salida del PCA pierde interpretabilidad en los features, lo cual dificulta el análisis, pero tienen que enfocarse en la visualización e intentar deducir el comportamiento.\n",
    "\n",
    "## Parte III - Aplicación de modelos no supervisados.\n",
    "\n",
    "Vamos a hacer unos análisis básicos con Kmeans y HDBSCAN y luego abrimos a investigar modelos que se adapten mejor al enfoque. \n",
    "\n",
    "### Consignas\n",
    "\n",
    "- Seleccionar un conjunto de features y aplicar:\n",
    "    - Kmeans\n",
    "    - HDBSCAN\n",
    "    - (opcional) AgglomerativeClustering\n",
    "- En todos los modelos que prueben, hagan dos o tres iteraciones, cambiando configuraciones de hiperparametros que produzcan resultados interesantes. ****************************************Analicen y describan los resultados. Incluyan en el análisis a los hiperparametros conocidos****************************************\n",
    "- Análisis de los resultados:\n",
    "    - Que resultados tuvieron? son buenos o malos? porque? Reflejan o no la realidad bajo algún punto de vista?\n",
    "    - Observen los hiperparametros. Cuales afectan mas en los resultados?\n",
    "- Repitan el análisis para otra selección de features (puede ser la misma selección pero [con o sin PCA](https://www.notion.so/Consignas-Practico-3-e4890c44e91346929d309d74cdb1a7fb?pvs=21))\n",
    "- Investiguen 1 o 2 modelos mas en internet, incluso pueden ver otros modelos que hayan sido usados en este mismo dataset (ver sitio en kaggle), también para encontrar clusters. Cuales modelos se acomodan mejor a este set de datos? La respuesta a esa pregunta va a depender de los features seleccionados.\n",
    "    - Elijan uno o dos modelos y apliquenlos al dataset. Justifiquen la elección.\n",
    "    - Visualicen los resultados. Describan lo que ven.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### Anexo I - Nota sobre PCA\n",
    "\n",
    "No es necesario que usen siempre el PCA para reducir el dataset. Por lo general se considera por la ultilidad en datasets con muchas columnas, pero tiene asociado el costo de perder la interpretabilidad de los resultados. Se puede, de todas maneras, aplicar el transformador inverso al resultado del modelo no supervisado para volver a obtener las variables originales y visualizar en que clusters quedaron. \n",
    "\n",
    "Lo mejor es probar la mayor cantidad de combinaciones posibles, sobre todo si los resultados no son buenos o son muy difíciles de interpretar. Usar PCA, no usarlo, usarlo pero invertirlo después, etcetera. En el informe pueden dejar solo las relevantes o las que se considere que da resultados mas interpretables y valiosos. Como siempre, ********************************************describan lo que ven.******************************************** Esta es la parte donde es mas valioso ser descriptivo, y cobra mucha importancia el análisis exploratorio para entender lo que pasa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neoland",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
